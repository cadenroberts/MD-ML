{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchmdnet.datasets.custom\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmdnet.module import LNNP\n",
    "from torch_geometric.loader import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "import yaml\n",
    "import numpy as np\n",
    "from torchmdnet.models.model import create_model\n",
    "import itertools\n",
    "from module import dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, embed, coord, batch_nums) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        embed = embed.flatten()\n",
    "        coord = coord.reshape(-1, coord.shape[-1])\n",
    "\n",
    "        # Increment the molecule id numbers in each batch after the first to\n",
    "        # produce a sequential numbering for the entire batch.\n",
    "        for i in range(1, len(batch_nums)):\n",
    "            offset = batch_nums[i - 1][-1] + 1\n",
    "            batch_nums[i] += offset\n",
    "        batch_nums = batch_nums.flatten()\n",
    "\n",
    "        energy, computed_force = self.model(embed, coord, batch_nums)\n",
    "        return energy, computed_force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(input_directory, model_directory, gpu_ids=None, batch_size=50):\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(123456)\n",
    "\n",
    "    # Enable deterministic behavior for CUDA operations\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Load the list of PDB files\n",
    "    with open(f\"{input_directory}/result/ok_list.txt\", \"r\") as file:\n",
    "        pdb_list = file.read().split(\"\\n\")\n",
    "\n",
    "    # Load only the first protein\n",
    "    pdb_list = pdb_list[:1]\n",
    "\n",
    "    # Load all data\n",
    "    all_data = dataset.ProteinDataset(input_directory, pdb_list)\n",
    "    num_proteins = all_data.num_proteins()\n",
    "    print(f\"Number of proteins in the dataset: {num_proteins}\")\n",
    "    data_loader = DataLoader(\n",
    "        all_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    checkpoint_path = f\"{model_directory}/checkpoint.pth\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = create_model(args=checkpoint[\"hyper_parameters\"])\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    # Initialize DataParallel\n",
    "    parallel_model = nn.DataParallel(BatchWrapper(model), device_ids=gpu_ids)\n",
    "    model = parallel_model.to(parallel_model.src_device_obj)\n",
    "\n",
    "    # Evaluation loop\n",
    "    parallel_model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    residue_losses = []\n",
    "\n",
    "    standard_residues = {\n",
    "        \"ALA\",\n",
    "        \"ARG\",\n",
    "        \"ASN\",\n",
    "        \"ASP\",\n",
    "        \"ASX\",\n",
    "        \"CYS\",\n",
    "        \"GLU\",\n",
    "        \"GLN\",\n",
    "        \"GLX\",\n",
    "        \"GLY\",\n",
    "        \"HIS\",\n",
    "        \"ILE\",\n",
    "        \"LEU\",\n",
    "        \"LYS\",\n",
    "        \"MET\",\n",
    "        \"PHE\",\n",
    "        \"PRO\",\n",
    "        \"SER\",\n",
    "        \"THR\",\n",
    "        \"TRP\",\n",
    "        \"TYR\",\n",
    "        \"VAL\",\n",
    "    }\n",
    "    amino_acid_mapping = {\n",
    "        name: index + 1 for index, name in enumerate(sorted(standard_residues))\n",
    "    }\n",
    "    reverse_amino_acid_mapping = {\n",
    "        index: name for name, index in amino_acid_mapping.items()\n",
    "    }\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Evaluation\", total=len(data_loader)):\n",
    "        coord, embed, force, batch_nums = batch\n",
    "        force = force.reshape(-1, force.shape[-1])\n",
    "        force = force.to(parallel_model.output_device)\n",
    "        _, out = parallel_model(embed, coord, batch_nums)\n",
    "        embed = embed.flatten()\n",
    "\n",
    "        for i in range(len(embed)):\n",
    "            residue_id = embed[i].item()\n",
    "            loss = criterion(force[i], out[i]).item()\n",
    "            residue_name = reverse_amino_acid_mapping.get(residue_id, \"UNKNOWN\")\n",
    "            residue_losses.append(\n",
    "                {\n",
    "                    \"frame_index\": i,\n",
    "                    \"residue_id\": residue_id,\n",
    "                    \"residue_name\": residue_name,\n",
    "                    \"loss\": loss,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return residue_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "\n",
    "# plot_residues.py /media/DATA_18_TB_1/daniel_s/cgschnet/cg_single_chain_CAprior_2024.04.03/\n",
    "# /media/DATA_18_TB_1/daniel_s/cgschnet/cgschnet_models/cg_single_chain_CAprior_2024.04.03/ --batch 32\n",
    "\n",
    "input_directory = (\n",
    "    \"/media/DATA_18_TB_1/daniel_s/cgschnet/cg_single_chain_CAprior_2024.04.03/\"\n",
    ")\n",
    "model_directory = \"/media/DATA_18_TB_1/daniel_s/cgschnet/cgschnet_models/cg_single_chain_CAprior_2024.04.03/\"\n",
    "residue_losses = val_model(\n",
    "    input_directory, model_directory, gpu_ids=None, batch_size=1\n",
    ")  # We are setting batch size to 1 to avoid batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(residue_losses)\n",
    "# write to txt\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "df.to_csv(\"residue_losses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(df):\n",
    "    # Filter the data to include only the first 50 frames\n",
    "    df_filtered = df.loc[df[\"frame_index\"] < 50].copy()\n",
    "    df_filtered.loc[:, \"frame_index\"] = df_filtered[\"frame_index\"].astype(int)\n",
    "\n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.barplot(\n",
    "        data=df_filtered, x=\"frame_index\", y=\"loss\", hue=\"residue_name\", errorbar=None\n",
    "    )\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlabel(\"Frame Index\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Per-Residue Losses\")\n",
    "    plt.legend(title=\"Residue Name\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the bar graph\n",
    "plot_performance(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Train a CGSchNet network\")\n",
    "    parser.add_argument(\"input\", help=\"Processed data to train on \")\n",
    "    parser.add_argument(\n",
    "        \"model\", default=None, nargs=\"?\", help=\"Checkpoint directory to continue\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gpus\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        help='List of GPUs to train on (e.g. \"0,1,2\") ',\n",
    "    )\n",
    "    parser.add_argument(\"--batch\", type=int, default=50, help=\"The batch size to use\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_path = args.input\n",
    "    model_path = args.model\n",
    "    if args.gpus:\n",
    "        gpu_ids = [int(i) for i in args.gpus.strip().split(\",\")]\n",
    "    else:\n",
    "        gpu_ids = None\n",
    "    batch_size = args.batch\n",
    "    val_model(\n",
    "        input_path, model_directory=model_path, gpu_ids=gpu_ids, batch_size=batch_size\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
