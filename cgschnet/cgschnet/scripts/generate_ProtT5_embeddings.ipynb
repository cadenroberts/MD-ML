{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mdtraj\n",
    "import tqdm\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "from moleculekit.molecule import Molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "\n",
    "`conda install transformers sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_T5_model():\n",
    "    model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")\n",
    "    model = model.to(device) # move model to GPU\n",
    "    model = model.eval() # set model to evaluation model\n",
    "    tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = get_T5_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_T5_embeddings(molecule, model, tokenizer):\n",
    "    # Generate the FASTA sequence for each chain (assumes 1 bead per residue)\n",
    "    fasta = \"\".join([i[-1] for i in molecule.atomtype])\n",
    "    fasta_list = []\n",
    "    for i in [len(list(i[1])) for i in itertools.groupby(molecule.segid)]:\n",
    "        fasta_list.append(fasta[:i])\n",
    "        fasta = fasta[i:]\n",
    "\n",
    "    # print(\"Generating ProtT5 embeddings for the sequences:\", \" \".join(fasta_list))\n",
    "\n",
    "    embedding_list = []\n",
    "    for fasta in fasta_list:\n",
    "        fasta = \" \".join(fasta)\n",
    "\n",
    "        token_encoding = tokenizer(fasta, add_special_tokens=True)\n",
    "        input_ids      = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(token_encoding['attention_mask']).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # The model expects a input of shape [batch, max_len]\n",
    "            embedding_repr = model(input_ids[None,:], attention_mask=attention_mask[None,:])\n",
    "            # Output has shape [batch, max_len, embedding_len]\n",
    "            # We also need to trim off the termination token, in the original script this was done with [:s_len] to\n",
    "            # also trim off the batch padding\n",
    "            embedding_list.append(embedding_repr.last_hidden_state[0][:-1])\n",
    "\n",
    "    embedding = torch.cat(embedding_list).cpu().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 182/987 [00:03<00:17, 46.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skip> result\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 331/987 [00:06<00:12, 54.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skip> priors.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 522/987 [00:10<00:09, 47.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skip> pdb_list.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 543/987 [00:11<00:09, 45.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skip> prior_fit_plots\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 602/987 [00:12<00:07, 49.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skip> prior_builder.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 661/987 [00:13<00:06, 51.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<skip> prior_params.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 987/987 [00:20<00:00, 48.29it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dir = \"/home/argon/Stuff/prot_trans/cg_raz081724_CA_lj_angleXCX_dihedralX_V1_opt\"\n",
    "for i in tqdm.tqdm(glob.glob(os.path.join(out_dir, \"*\"))):\n",
    "    pdbid = os.path.basename(i)\n",
    "    if not os.path.exists(os.path.join(out_dir, f\"{pdbid}/raw/\")):\n",
    "        tqdm.tqdm.write(f\"<skip> {pdbid}\")\n",
    "        continue\n",
    "    # This assumes 1 the mapping uses bead per residue\n",
    "    molecule = Molecule(os.path.join(out_dir, f\"{pdbid}/processed/{pdbid}_processed.psf\"))\n",
    "    embedding = gen_T5_embeddings(molecule, model, tokenizer)\n",
    "\n",
    "    outpath = os.path.join(out_dir, f\"{pdbid}/raw/protT5_embedding.npy\")\n",
    "    np.save(outpath, embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
